<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Primary Meta Tags -->
    <title>Does CLIP's generalization performance mainly stem from high train-test similarity?</title>
    <meta name="title" content="Does CLIP's generalization performance mainly stem from high train-test similarity?">
    <meta name="description"
        content="Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful CLIP's high zero-shot performance is as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's performance, and other properties of the training data must drive CLIP to learn good representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original performance.">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://brendel-group.github.io/clip-ood">
    <meta property="og:title" content="Does CLIP's generalization performance mainly stem from high train-test similarity?">
    <meta property="og:description"
        content="Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful CLIP's high zero-shot performance is as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's performance, and other properties of the training data must drive CLIP to learn good representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original performance.">
    <meta property="og:image" content="https://brendel-group.github.io/clip-ood/img/fig1.svg">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://brendel-group.github.io/clip-ood">
    <meta property="twitter:title" content="Does CLIP's generalization performance mainly stem from high train-test similarity?">
    <meta property="twitter:description"
        content="Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful CLIP's high zero-shot performance is as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's performance, and other properties of the training data must drive CLIP to learn good representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original performance.">
    <meta property="twitter:image" content="https://brendel-group.github.io/clip-ood/img/fig1.svg">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
        integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>

    <style>
        .main {
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        .code {
            font-family: 'IBM Plex Mono', monospace;
        }

        h3 {
            margin-top: 1.0rem; 
        }

        .row-dense {
            padding-bottom: 0;
        }

        .a {
            color: gainsboro;
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        td {
            padding: 0 15px;
        }

        p {
            text-align: justify;

        }

        .collapse-container {
            text-align: center;
            position: relative;

        }

        .collapse-container #moreless.collapsed:after {
            content: '+ Show More';
        }

        .collapse-container #moreless:not(.collapsed):after {
            content: '- Show Less';
        }

        .collapse-container .collapse.collapse:not(.show) {
            display: block;
            /* height = lineheight * no of lines to display */
            height: 7.7em;
            overflow: hidden;
        }

        .collapse-container .collapse.collapse:not(.show):before {
            content: '';
            width: 100%;
            height: 7.7em;
            position: absolute;
            left: 0;
            top: 0;
            background: linear-gradient(rgba(255, 255, 255, 0), 60px, white);
        }

        .collapse-container .collapse.collapsing {
            height: 7.7em;
        }
    </style>

    <title>Does CLIP's generalization performance mainly stem from high train-test similarity?</title>
</head>

<body>
    <div class="container main">
        <div class="row">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8" id="main-content">
                <div class="row text-center my-5" id="#">
                    <h1>Does CLIP's generalization performance mainly stem from high train-test similarity?</h1>
                </div>

                <!-- Begin author list-->
                <div class="row text-center mb-4">
                    <div class="col-sm-4 mb-4">
                        Prasann Mayilvahanan*
                        <a href="prasanna.mayilvahanan@uni-tuebingen.de"><i class="far fa-envelope"></i></a><br>
                        University of Tübingen, MPI-IS, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Thaddäus Wiedemer*
                        <a href="mailto:thaddaeus.wiedemer@gmail.com"><i class="far fa-envelope"></i></a>
                        <a href="https://twitter.com/thwiedemer"><i class="fab fa-x-twitter"></i></a><br>
                        MPI-IS, University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Evgenia Rusak
                        <a href="evgenia.a.rusak@gmail.com"><i class="far fa-envelope"></i></a><br>
                        University of Tübingen, MPI-IS, TÜbingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Matthias Bethge
                        <a href="mailto:matthias@bethgelab.org"><i class="far fa-envelope"></i></a><br>
                        University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Wieland Brendel
                        <a href="mailto:wieland.brendel@tue.mpg.de"><i class="far fa-envelope"></i></a><br>
                        MPI-IS, ELLIS Institute Tübingen, Tübingen AI Center
                    </div>
                </div>
                <!-- End author list-->

                <div class="row text-center">
                    <div class="col-sm-4 mb-4 offset-sm-2">
                        <h4>
                            <a href="https://arxiv.org/abs/2310.09562" target="_blank">
                                <i class="fas fa-file-alt"></i>
                                Paper
                            </a>
                        </h4>
                    </div>
                    <div class="col-sm-4 mb-4">
                        <h4>
                            <a href="https://github.com/brendel-group/clip-ood" target="_blank">
                                <i class="fab fa-github"></i>
                                Code
                            </a>
                        </h4>
                    </div>
                </div>

                <div class="row text-center">
                    <p>
                        <b>tl;dr:</b>
                        <span class="text-muted">
                            CLIP's ability to generalize to standard OOD benchmarks does not mainly stem from highly similar images in its training dataset.
                        </span>
                    </p>
                </div>

                <div class="row mt-2">
                    <h3>News</h3>
                </div>

                <div class="row">
                    <table>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Feb '24</span>
                            </td>
                            <td>
                                Our paper was accepted at <a href="https://openreview.net/forum?id=tnBaiidobu" target="_blank">ICLR 2024</a>!
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Feb '24</span>
                            </td>
                            <td>
                                An earlier version of our paper was accepted at <a href="https://openreview.net/forum?id=Og4BHvCA8h" target="_blank">the NeurIPS 2023 DistShift workshop</a>!
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Oct '23</span>
                            </td>
                            <td>
                                The pre-print is now available on <a href="https://arxiv.org/abs/2310.09562" target="_blank">arXiv</a>.
                            </td>
                        </tr>
                    </table>
                </div>

                <div class="row mt-2">
                    <div class="col-12">
                        <p>
                        </p>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Abstract</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12 collapse-container">
                        <p class="collapse" id="abstractText" aria-expanded="false">
                            Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful CLIP's high zero-shot performance is as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's performance, and other properties of the training data must drive CLIP to learn good representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original performance
                        </p>
                       <a role="button" id="moreless" class="collapsed" data-toggle="collapse" href="#abstractText" aria-expanded="false" aria-controls="abstractText"></a>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Overview</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            Object-centric representations encode each object in a sence in a separate slot.
                            Because of this, they are thought to generalize compositionlly, which means an object-centric model should be able to extract the correct representation for unseen combinations of observed objects.
                            However, in practice, when we train an object-centric autoencoder on a training set that only contains some object combinations, we find that the model performs well for seen combinations, but not for unseen ones.
                        </p>
                        <p>   
                            We pose compositional generalization as a slot identifiability problem: Is the model able to reconstruct the ground-truth slot on the training set?
                            If so, does this ability generalize to unseen combinations of objects?
                        </p>
                        <p>
                            We show that if the training set is chosen correctly, if the decoder of the model is additive, and if we train with an additional loss term that we dub <em>compositional consistency loss</em>, the model generalizes compositionally in this sense.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig1_v12.svg" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                            <p>
                                We understand compositional generalization in object-centric learning as the ability of a model to represent the objects in a scene in distinct slots, even for unseen object combinations.
                                By plotting the reconstruction error over the entire domain, we see that an additive decoder by itself generates valid images within and outside of the training set, that is, it generalizes (A).
                                But put together with the encoder, the reconstruction error shoots up outside of the training domain—the model is unable to generalize (B).
                                Only if we add our proposed <em>compositional consistency</em> objective that aligns the encoder with the decoder is the entire mdoel able to generalize (C).
                            </p>
                        </small>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>How do we formalize compositional generalization and which assumptions do we make?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            We start from a result from <a href="https://brendel-group.github.io/objects-identifiability/" data-toggle="tooltip" title data-original-title="Brady et al. 2023. Provably Learning Object-Centric Representations.">our earlier work</a> that shows that a model can correctly identify the slots of a assumed ground-truth data-generating process (that is, it can learn object-centric representations) if the model data-generation process is <em>compositional</em> and <em>irreducible</em> (both temrs introduced in our earlier work) and the model is trained on the entire domain.
                            From there, we take four steps to arrive at our final result:
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig2_v10.svg" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <ol>
                            <li>We assume all the training data is generated from a <em>slot-supported subset</em> of the data domain, which is any subset that contains all variations of each individual object, but not necessarily all combinations of all objects.</li>
                            <li>We extend our earlier result to show that a <em>compositional</em> autoencoder trained via a reconstruction objective can still identify the ground-truth slots on this training domain if the slot-supported subset is also convex. Note that the recovered training subset of the latent space (blue) is not exactly the same as the ground-truth, since the model recovers it only up to some permissible ambiguities.</li>
                            <li>Since the model recovered the individual slots, we can simply recombine them to obtain unseen combinations of objects in the model's latent space. We show that if the decoder is additive, its reconstructions of these unseen combinations correspond exactly to images of unseen object combinations.</li>
                            <li>However, for unseen object combinations, the encoder initially does not produce representations that can be expressed by a recombination of slots in the models latent space. We therefore need to align the encoder output with the expected input to the decoder. We achieve this via our proposed <em>compositional consistency loss</em>.</li>
                        </ol>
                        <p>
                            We arrive at a model whose representations slot-identify the ground-truth latent space, even for unseen combinations of objects.
                        </p>
                    </div>
                </div>


                <div class="row mt-2">
                    <h3>What does this look like in practice?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            From an implementation perspective, our biggest contribution is the compositional consistency loss that is included as an additional training objective to the default reconstruction loss.
                            We can implement by taking the model's latent representation of the batch during the forward pass, randomly recombining slots between samples, and then applying the decoder and encoder to it. The resulting latent representation of each sample should be consistent with the recombination of latents, which we can measure, e.g., via a L2-loss.
                        </p>
                        <p>
                            In latent space, recombining the slots of two random samples will most likely lead to a combination of slots that is not part of the training domain. The decoder can produce a valid image of this slot combination since it is additive and therefore invariant to the order and combination of slots.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig3_v6.svg" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                            The compositional consistency loss enforces cycle-consistency in the model's latent space after slot-wise recombinations, similar to the cycle-consistency in image space enforced by the default reconstruction loss (left).
                            Most recombinations of slots between samples in a batch correspond to object combinations outside of the training domain (right).
                        </small>
                        <p>
                            We can show that the model slot-identifies the ground truth exactly when both losses are minimized.
                            Additionally, we do not explicitly need to optimize for compositionality of the decoder, as this property is implicitly enforced during training.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig4_v5.svg" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                             We train multiple models in a controlled setting and see that models slot-identify if they minimize both training objectives (left) and that  compositionality is implicitly satisfied (right).
                        </small>
                    </div>
                </div>


                <div class="row mt-2">
                    <h3>What does this mean for object-centric methods like Slot Attention?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            The popular object-centric method <a href="https://arxiv.org/abs/2006.15055" target="_blank" data-toggle="tooltip" title data-original-title="Locatello et al. 2020. Object-Centric Learning with Slot Attention">Slot Attention</a> fails to generalize compositionally <out-of-the-box class=""></out-of-the-box>
                            Given our theoretical insights, we can understand what's going wrong.
                        </p>
                        <p>
                            First, its decoder is not additive since it uses a softmax operation across slots.
                            Therefore, the decoder does not generalize.
                            Replacing the softmax with a slot-wise sigmoid fixes this issue.
                        </p>
                        <p>
                            Second, as with a vanilla autoencoder, we need to add our compositional consistency training objective to enable the whole model to generalize.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig5_v11.svg" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                            Slot Attention fails to generalize compositionally out-of-the-box, but modifying it to satisfy additivity and training with the compositional consistency loss alleviates this.
                        </small>
                    </div>
                </div>

                <div class="row">
                    <h3>Acknowledgements & Funding</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12 collapse-container">
                        <p class="collapse" id="acknowledgmentsText" aria-expanded="false">
                            We would like to thank (in alphabetical order): Thomas Klein, George Pachitariu, Matthias Tangemann, Vishaal Udandarao, Max Wolff, and Roland Zimmermann for helpful discussions, feedback, and support with setting up the experiments.<br>
                            This work was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. WB acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded by the Good Ventures Foundation. WB is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645. This research utilized compute resources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.<br>
                            We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting PM, TW, and ER.
                        </p>
                       <a role="button" id="moreless" class="collapsed" data-toggle="collapse" href="#acknowledgmentsText" aria-expanded="false" aria-controls="acknowledgmentsText"></a>
                    </div>
                </div>
                <div class="row">
                    <h3>BibTeX</h3>
                </div>
                <div class="row">
                    <p>If you find our study helpful, please cite our paper:</p>
                </div>
                <div class="row justify-content-md-center">
                    <div class="col-sm-12 rounded p-3 m-2" style="background-color:lightgray;">
                        <small class="code">
                            @inproceedings{mayilvahanan2024does,<br>
                                &nbsp;&nbsp;title={Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?},<br>
                                &nbsp;&nbsp;author={<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Prasanna Mayilvahanan and Thadd{\"a}us Wiedemer and Evgenia Rusak and Matthias Bethge and Wieland Brendel<br>
                                &nbsp;&nbsp;},<br>
                                &nbsp;&nbsp;booktitle={The Twelfth International Conference on Learning Representations},<br>
                                &nbsp;&nbsp;year={2024},<br>
                                &nbsp;&nbsp;url={https://openreview.net/forum?id=tnBaiidobu}<br>
                            }
                        </small>
                    </div>
                </div>

                <div class="row">
                    <small class="text-muted">Webpage designed using Bootstrap 4.5.</small>
                    <a href="#" class="ml-auto"><i class="fas fa-sort-up"></i></a>
                </div>

            </div>
        </div>

    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>

</body>

</html>

</html>
